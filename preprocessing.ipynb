{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation, digits, ascii_lowercase\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import (train_test_split,cross_val_score,\n",
    "                                      StratifiedShuffleSplit)\n",
    "                                      \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp = reviews.sample(n=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define lists of escape sequences, digits, stopwords to use for parseing. Also defines the type of stemming to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "escapes = ''.join([chr(char) for char in range(1, 32)])\n",
    "removeables = escapes + digits \n",
    "stops = [str(word) for word in stopwords.words('english')] + list(ascii_lowercase)\n",
    "sno = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function to process the text and then use it to make a new coloumn in the dataframe where all texted has been processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_text(text, stem=True):\n",
    "    ''' This function takes a review string and removes all escape sequences,\n",
    "        digits, punctuation, http links, and stop words. Furthermore, every\n",
    "        word in the string will be stemmed using nltk's snowball stemmer.\n",
    "        Every word is also transformed to be lowercase.'''\n",
    "    \n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    regex = re.compile('[%s]' % re.escape(punctuation))\n",
    "    text = regex.sub(' ', text)\n",
    "    text = text.translate(None, removeables)\n",
    "    text = text.decode('utf8')\n",
    "    if stem == True:\n",
    "        text = ' '.join([sno.stem(word.lower()) for word in text.split() if word.lower() not in set(stops)])\n",
    "    else:\n",
    "        text = ' '.join([word.lower() for word in text.split() if word.lower() not in set(stops)])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp['parsed_text']=samp.text.apply(parse_text,stem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words, tf-idf vectorization\n",
    "Create the bag of words representation. Find counts of each word in document and in whole courpus. Then create the tfidf representation. Worth also considering binary count vecorizing, supposed to work better for smaller sample sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = samp.parsed_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(words) = 20197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars = samp.stars.reshape(10000,1)\n",
    "counts = X_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = np.concatenate((stars,counts),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns = ['star_rating'] + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "star_counts = dict(samp.stars.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = df.groupby(['star_rating']).sum().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,v in star_counts.items():\n",
    "    groups[k] = groups[k]/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinstewart/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>star_rating</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.114424</td>\n",
       "      <td>0.237089</td>\n",
       "      <td>0.353175</td>\n",
       "      <td>0.539394</td>\n",
       "      <td>0.583479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>0.500403</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.584921</td>\n",
       "      <td>0.574242</td>\n",
       "      <td>0.496132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.793427</td>\n",
       "      <td>0.680159</td>\n",
       "      <td>0.562121</td>\n",
       "      <td>0.436736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.280419</td>\n",
       "      <td>0.637324</td>\n",
       "      <td>0.867460</td>\n",
       "      <td>0.721970</td>\n",
       "      <td>0.389069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.445608</td>\n",
       "      <td>0.444836</td>\n",
       "      <td>0.367460</td>\n",
       "      <td>0.307197</td>\n",
       "      <td>0.329174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service</th>\n",
       "      <td>0.438356</td>\n",
       "      <td>0.484742</td>\n",
       "      <td>0.353175</td>\n",
       "      <td>0.313258</td>\n",
       "      <td>0.316446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.402901</td>\n",
       "      <td>0.610329</td>\n",
       "      <td>0.541270</td>\n",
       "      <td>0.430682</td>\n",
       "      <td>0.309209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.467365</td>\n",
       "      <td>0.471831</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.284253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0.064464</td>\n",
       "      <td>0.123239</td>\n",
       "      <td>0.134127</td>\n",
       "      <td>0.158712</td>\n",
       "      <td>0.282256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.044319</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.167045</td>\n",
       "      <td>0.273272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>0.509267</td>\n",
       "      <td>0.525822</td>\n",
       "      <td>0.441270</td>\n",
       "      <td>0.348106</td>\n",
       "      <td>0.267781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>back</th>\n",
       "      <td>0.432716</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>0.268254</td>\n",
       "      <td>0.265909</td>\n",
       "      <td>0.250562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>0.372280</td>\n",
       "      <td>0.373239</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.295833</td>\n",
       "      <td>0.246319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>0.138598</td>\n",
       "      <td>0.215962</td>\n",
       "      <td>0.253175</td>\n",
       "      <td>0.273864</td>\n",
       "      <td>0.226354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>0.195004</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.329167</td>\n",
       "      <td>0.223609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ve</th>\n",
       "      <td>0.183723</td>\n",
       "      <td>0.221831</td>\n",
       "      <td>0.215079</td>\n",
       "      <td>0.218561</td>\n",
       "      <td>0.216870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0.024980</td>\n",
       "      <td>0.029343</td>\n",
       "      <td>0.053968</td>\n",
       "      <td>0.103409</td>\n",
       "      <td>0.213876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>0.066882</td>\n",
       "      <td>0.104460</td>\n",
       "      <td>0.119841</td>\n",
       "      <td>0.182197</td>\n",
       "      <td>0.211380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.514102</td>\n",
       "      <td>0.467136</td>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.271212</td>\n",
       "      <td>0.196406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>0.159549</td>\n",
       "      <td>0.180751</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.221591</td>\n",
       "      <td>0.194659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friendly</th>\n",
       "      <td>0.033844</td>\n",
       "      <td>0.109155</td>\n",
       "      <td>0.103175</td>\n",
       "      <td>0.161364</td>\n",
       "      <td>0.176940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>0.331990</td>\n",
       "      <td>0.266432</td>\n",
       "      <td>0.172222</td>\n",
       "      <td>0.174242</td>\n",
       "      <td>0.173946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staff</th>\n",
       "      <td>0.136180</td>\n",
       "      <td>0.134977</td>\n",
       "      <td>0.130159</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.161218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delicious</th>\n",
       "      <td>0.011281</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.075397</td>\n",
       "      <td>0.160985</td>\n",
       "      <td>0.159970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>0.094279</td>\n",
       "      <td>0.228873</td>\n",
       "      <td>0.308730</td>\n",
       "      <td>0.269318</td>\n",
       "      <td>0.159471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vegas</th>\n",
       "      <td>0.103948</td>\n",
       "      <td>0.136150</td>\n",
       "      <td>0.117460</td>\n",
       "      <td>0.135606</td>\n",
       "      <td>0.158972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>0.364222</td>\n",
       "      <td>0.340376</td>\n",
       "      <td>0.226984</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.153481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.199839</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.137879</td>\n",
       "      <td>0.150736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>definitely</th>\n",
       "      <td>0.038678</td>\n",
       "      <td>0.092723</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>0.171970</td>\n",
       "      <td>0.148490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>try</th>\n",
       "      <td>0.105560</td>\n",
       "      <td>0.160798</td>\n",
       "      <td>0.195238</td>\n",
       "      <td>0.171591</td>\n",
       "      <td>0.147492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "star_rating         1         2         3         4         5\n",
       "great        0.114424  0.237089  0.353175  0.539394  0.583479\n",
       "place        0.500403  0.647887  0.584921  0.574242  0.496132\n",
       "food         0.534247  0.793427  0.680159  0.562121  0.436736\n",
       "good         0.280419  0.637324  0.867460  0.721970  0.389069\n",
       "time         0.445608  0.444836  0.367460  0.307197  0.329174\n",
       "service      0.438356  0.484742  0.353175  0.313258  0.316446\n",
       "like         0.402901  0.610329  0.541270  0.430682  0.309209\n",
       "one          0.467365  0.471831  0.383333  0.358333  0.284253\n",
       "best         0.064464  0.123239  0.134127  0.158712  0.282256\n",
       "love         0.044319  0.116197  0.114286  0.167045  0.273272\n",
       "get          0.509267  0.525822  0.441270  0.348106  0.267781\n",
       "back         0.432716  0.408451  0.268254  0.265909  0.250562\n",
       "go           0.372280  0.373239  0.328571  0.295833  0.246319\n",
       "also         0.138598  0.215962  0.253175  0.273864  0.226354\n",
       "really       0.195004  0.366197  0.404762  0.329167  0.223609\n",
       "ve           0.183723  0.221831  0.215079  0.218561  0.216870\n",
       "amazing      0.024980  0.029343  0.053968  0.103409  0.213876\n",
       "always       0.066882  0.104460  0.119841  0.182197  0.211380\n",
       "would        0.514102  0.467136  0.419048  0.271212  0.196406\n",
       "well         0.159549  0.180751  0.205556  0.221591  0.194659\n",
       "friendly     0.033844  0.109155  0.103175  0.161364  0.176940\n",
       "even         0.331990  0.266432  0.172222  0.174242  0.173946\n",
       "staff        0.136180  0.134977  0.130159  0.145833  0.161218\n",
       "delicious    0.011281  0.035211  0.075397  0.160985  0.159970\n",
       "nice         0.094279  0.228873  0.308730  0.269318  0.159471\n",
       "vegas        0.103948  0.136150  0.117460  0.135606  0.158972\n",
       "us           0.364222  0.340376  0.226984  0.166667  0.153481\n",
       "first        0.199839  0.227700  0.183333  0.137879  0.150736\n",
       "definitely   0.038678  0.092723  0.109524  0.171970  0.148490\n",
       "try          0.105560  0.160798  0.195238  0.171591  0.147492"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.sort([5],ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfs = X_tfidf.toarray()\n",
    "T = np.concatenate((stars,tfidfs),axis=1)\n",
    "df_tfidf = pd.DataFrame(T)\n",
    "df_tfidf.columns = ['star_rating'] + words\n",
    "tfidf_groups = df.groupby(['star_rating']).sum().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in star_counts.items():\n",
    "    tfidf_groups[k] = tfidf_groups[k]/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfs[:5][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array(samp.stars.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split\n",
    "\n",
    "Need to split the dataset into train and test, then use cross val to traing the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.30,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(y_train, 5, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb = MultinomialNB().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(preds).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(y_test).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importnatn to consider what is the best accuracy measure to use to test results. notice that star rating is a ordered "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
